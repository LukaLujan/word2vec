{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gthb2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMg+DEctongX5H155fKV1ay",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukaLujan/word2vec/blob/main/gthb2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTzMUGQ1blcH",
        "outputId": "3416ca06-477a-45b7-b13b-06ece5852a01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#This notebook is very similar like one before, so there will be less comments. \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "import gensim.downloader as api\n",
        "from google.colab import drive\n",
        "\n",
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "import re\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This where I am going to mount my drive on google colab\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1WcaAd9bxb4",
        "outputId": "3d28774c-c048-464b-eeb1-a6f5906927c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#I am changing directory , however you may do all of this on different way. \n",
        "%cd drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCM9toAfb9oK",
        "outputId": "29b85539-27bd-422b-9f7a-9a1b8f255b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#i have this predownloaded on my google drive. This is a link and code to download pretrained model. \n",
        "\n",
        "#!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "metadata": {
        "id": "YwIgAta9cZm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_0QCTbC6ceYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how you activate your model. notice that it will take huge % of your google colab RAM. We will have to deal with it.\n",
        "\n",
        "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec_model = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "metadata": {
        "id": "hKAQxEBrchaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v_apple = word2vec_model[\"apple\"] \n",
        "\n",
        "print(v_apple.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onR1cKwidJye",
        "outputId": "bb72051d-2b41-4486-b8ef-6235cc678ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "word2vec_model[\"apple\"]\n"
      ],
      "metadata": {
        "id": "yKga7StpdQoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lbP1y-DEtv_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#word2vec_model[\"breitbart\"]   -not in vocab"
      ],
      "metadata": {
        "id": "OWSP7rKndNxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now process is almost the same as in the notebook before.\n",
        "df =pd.read_csv(\"train.csv\")\n",
        "df=df.dropna()"
      ],
      "metadata": {
        "id": "NzcNdkqPdPTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "JwqT6-HU6m2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKO3dHbL6wnd",
        "outputId": "2ae7bd74-6e83-4e29-d0a9-7d0c75b54a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "j8lbJRFt-x8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess our \"titles\" - we dont need to preprocess df[\"text\"] because our model is already pre-trained.\n",
        "messages_text = []\n",
        "\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "for par in df[\"title\"].values:\n",
        "    tmp = []\n",
        "    sentences = nltk.sent_tokenize(par)\n",
        "     \n",
        "    for sent in sentences:\n",
        "      \n",
        "        sent = sent.lower()\n",
        "        \n",
        "        tokens = tokenizer.tokenize(sent)\n",
        "         \n",
        "\n",
        "        filtered_words = [w.strip() for w in tokens if w not in STOPWORDS and len(w) > 1]\n",
        "        filtered_words2 = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
        "        tmp.extend(filtered_words2)\n",
        "    messages_text.append(tmp)"
      ],
      "metadata": {
        "id": "jh0o-4iv4RN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ones_vector(d):\n",
        "  return np.ones(d)\n",
        "  "
      ],
      "metadata": {
        "id": "y-KGmKy6708T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(messages_text)):\n",
        "  messages_text[i] =\" \".join(word for word in messages_text[i])"
      ],
      "metadata": {
        "id": "yn_ZkHfz8DZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ajn6pWol-OuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lista_vectora =[]\n",
        "for row in messages_text:\n",
        "  temp_ls_vec=[]\n",
        "  for token in row.split():\n",
        "    if token in word2vec_model.vocab:\n",
        "      temp_ls_vec.append(word2vec_model[token])\n",
        "    else:\n",
        "      temp_ls_vec.append(ones_vector(300))\n",
        "  lista_vectora.append(temp_ls_vec)"
      ],
      "metadata": {
        "id": "4GDpeMas8LlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yQXLWPTz-Tuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len =[]\n",
        "for sent in lista_vectora:\n",
        "  max_len.append(len(sent))\n",
        "print(max(max_len))\n",
        "print(len(max_len))\n",
        "L = max(max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3kwo65Y9pEJ",
        "outputId": "d1f1eade-547d-4e20-f476-47cb2691b550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n",
            "18285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def null_vector(num):\n",
        "  return np.zeros(num)"
      ],
      "metadata": {
        "id": "Y0yPb00eo0VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_embeddings = []\n",
        "for row in lista_vectora:\n",
        "  temp_zer=[]\n",
        "  if len(row) <L:\n",
        "    for i in range(L-len(row)):\n",
        "      temp_zer.append(null_vector(300))\n",
        "  padded_embeddings.append(temp_zer)  "
      ],
      "metadata": {
        "id": "hKsryc0To5o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_vectors= [ k+v for k,v in zip(lista_vectora ,padded_embeddings )]"
      ],
      "metadata": {
        "id": "6kva-3uQo0ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(padded_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvdzJGhCpCYN",
        "outputId": "c8e1b348-8d7c-419a-b6ac-29598014dfbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18285"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_train = np.array(df[\"label\"])\n",
        "X_train = np.array(padded_vectors)"
      ],
      "metadata": {
        "id": "nK-SCHYepEG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUo40_qZpaBd",
        "outputId": "7edded75-48ca-4e01-e51c-fb895b76a0ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18285, 47, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_shape =(X_train.shape[1], X_train.shape[2])"
      ],
      "metadata": {
        "id": "xuY0UVO6pXTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "aiVN4qQSpfRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Spliting data into validation and train. Test data will come latter.\n",
        "X1_train, X1_val, y1_train, y1_val  =train_test_split(X_train, y_train, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "UT8wxuy0phle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, LSTM, SpatialDropout1D, Dropout, TimeDistributed, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "SHKVhzYvpj2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "# model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "    # here we do not need Embedding layer because we did it on our own) \n",
        "model.add(Input(shape=input_shape))\n",
        "    # https://keras.io/api/layers/core_layers/input/\n",
        "\n",
        "#This is my simple network that will yield some results. Goal here was more on preprocessing, embedding and how to use Word2Vec.\n",
        "#You can play with tunning \n",
        "\n",
        "model.add(LSTM(64))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(32))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iimmxeBEpltF",
        "outputId": "94f3747c-3dc7-426b-bc22-fa02e03d7e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 64)                93440     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 95,553\n",
            "Trainable params: 95,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I am deleting all those variables to release RAM. Otherwise my google-colab 12GB RAM will crash. \n",
        "del messages_text, padded_embeddings, padded_vectors, df, lista_vectora, X_train, y_train"
      ],
      "metadata": {
        "id": "4H9U5zvlp-7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.fit(X1_train,y1_train, validation_data=(X1_val, y1_val), epochs=5, batch_size=32, callbacks=[EarlyStopping(monitor='val_loss', patience=2,restore_best_weights=True)] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bvp8q0bhprgM",
        "outputId": "8d511bd3-5626-4b4e-d776-80df76a58596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "383/383 [==============================] - 22s 51ms/step - loss: 0.3846 - accuracy: 0.8218 - val_loss: 0.3203 - val_accuracy: 0.8767\n",
            "Epoch 2/5\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 0.2715 - accuracy: 0.8887 - val_loss: 0.3089 - val_accuracy: 0.8560\n",
            "Epoch 3/5\n",
            "383/383 [==============================] - 19s 50ms/step - loss: 0.2316 - accuracy: 0.9038 - val_loss: 0.2529 - val_accuracy: 0.8936\n",
            "Epoch 4/5\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 0.2086 - accuracy: 0.9109 - val_loss: 0.2361 - val_accuracy: 0.8996\n",
            "Epoch 5/5\n",
            "383/383 [==============================] - 19s 50ms/step - loss: 0.1912 - accuracy: 0.9215 - val_loss: 0.2508 - val_accuracy: 0.9024\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fad0cf6d150>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing our data. Notace that  y -values are in separated csv file(\"submit.csv\")\n",
        "df_Xtest = pd.read_csv(\"test.csv\")\n",
        "df_ytest =pd.read_csv(\"submit.csv\")"
      ],
      "metadata": {
        "id": "h5Y-oMtsr0Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#concating two separated test data frames so I can for example get rid of rows with missing values much easier.\n",
        "df_Xy = pd.concat([df_Xtest, df_ytest], axis=1)"
      ],
      "metadata": {
        "id": "SvwFvPjur0md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_Xy.dropna(inplace=True)\n",
        "df_Xy.reset_index(inplace=True)\n",
        "print(len(df_Xy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kaafg3wr0ps",
        "outputId": "e00ef49d-c3a7-4cff-b8a5-2a986674eed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test data must pass same process as train data. Everything must be preprocessed and tokens converted into 300 dimensional vectors.\n",
        "\n",
        "messagesX = []\n",
        "\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "for par in df_Xy[\"title\"].values:\n",
        "    tmp = []\n",
        "    sentences = nltk.sent_tokenize(par)\n",
        "    for sent in sentences:\n",
        "        sent = sent.lower()\n",
        "        tokens = tokenizer.tokenize(sent)\n",
        "        filtered_words = [w.strip() for w in tokens if w not in STOPWORDS and len(w) > 1]\n",
        "        filtered_words2 = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
        "        tmp.extend(filtered_words2)\n",
        "    messagesX.append(tmp)"
      ],
      "metadata": {
        "id": "kgMYkZDor0tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messagesX[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hpUW8oOr0wE",
        "outputId": "eb1988e9-3f82-40d7-ebef-8aeba83b7515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['specter',\n",
              " 'trump',\n",
              " 'loosens',\n",
              " 'tongue',\n",
              " 'purse',\n",
              " 'string',\n",
              " 'silicon',\n",
              " 'valley',\n",
              " 'new',\n",
              " 'york',\n",
              " 'time']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(messagesX)):\n",
        " messagesX[i] =\" \".join(word for word in messagesX[i]) "
      ],
      "metadata": {
        "id": "LJuhALpgr8m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if the word from data set is part of our vocab(that is trained on text data ) it will get it's 300 dimension vector. Otherwise it gets vector of ones.\n",
        "lista_vectora2 =[]\n",
        "for row in messagesX:\n",
        "  temp_ls_vec=[]\n",
        "  for token in row.split():\n",
        "    if token in word2vec_model.wv.vocab:\n",
        "      temp_ls_vec.append(word2vec_model.wv[token])\n",
        "    else:\n",
        "      temp_ls_vec.append(ones_vector(300))\n",
        "  lista_vectora2.append(temp_ls_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpIVEOFxr8qe",
        "outputId": "c2c14b53-43c1-4c60-a65f-604eeb184e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  import sys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Everything else that is \"empty space\" will be padded with zeros.\n",
        "padded_embeddings2 = []\n",
        "for row in lista_vectora2:\n",
        "  temp_zer=[]\n",
        "  if len(row) <L:\n",
        "    for i in range(L-len(row)):\n",
        "      temp_zer.append(null_vector(300))\n",
        "  padded_embeddings2.append(temp_zer) "
      ],
      "metadata": {
        "id": "93tiCZ5Rr8t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#as with train data, we do same with test data\n",
        "padded_vectors2= [ k+v for k,v in zip(lista_vectora2 ,padded_embeddings2 )]"
      ],
      "metadata": {
        "id": "7QrxaPXGr-vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.array(df_Xy[\"label\"])\n",
        "X_test = np.array(padded_vectors2)"
      ],
      "metadata": {
        "id": "JMpYaiGUr-ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting results\n",
        "y_preds = model.predict(X_test)"
      ],
      "metadata": {
        "id": "f1lhPWOIr-1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#because I used sigmoid function for predictions, everything above 0.5 will be classifed as fake news, everything bellow 0.5 as reliable news source\n",
        "for i in range(len(y_preds)):\n",
        "  if y_preds[i] >=0.5:\n",
        "    y_preds[i]=1\n",
        "  else:\n",
        "    y_preds[i] =0"
      ],
      "metadata": {
        "id": "iVth4FJtsGdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets see how our data is split\n",
        "y_preds=y_preds.flatten()\n",
        "pd.Series(y_preds).value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTPYopWbsHE9",
        "outputId": "0cc73ea7-0d9d-49f9-f5b2-0ca69c69fbbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    2677\n",
              "1.0    1898\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "2aZGqZf-sIW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#as we can see, accuracy is even lower in this huge pre-trained model.\n",
        "print(accuracy_score(y_test, y_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1uIVnl1sKTF",
        "outputId": "f87c6e3e-85e0-473c-d763-ee0a56baabe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6192349726775956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In next notebook we will combine word2vec model that will train and learn on our corpus and with pre-trained word2vec model. "
      ],
      "metadata": {
        "id": "VgHkKsWG3pIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hKmT3tY64BN4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}