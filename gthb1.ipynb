{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gthb1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMohEZI6dWTS4fTb8k/k8e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukaLujan/word2vec/blob/main/gthb1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this google colab notebook, I am going to show how you can use Gensim Word2Vec model to do word embedding on your own dictionary , so it will give us best possible vectors that we can use them latter for training our model on train set, and of course test it on test set data.  Watching various code onthe internet I found that it is very easy to find someone using Word2Vec model, however interestingly vast majority of code skip testing model on the real test data. Literally I couldn't find anything so with some effort I made my own process. People on Kaggle just get some \"fancy\" 96% accuracy rate on validation  and that's it. \n",
        "We are using \"fake news data set\" from here; \n",
        "https://www.kaggle.com/c/fake-news/data\n",
        "\n",
        "Here you have a train.csv , test.csv and submit.csv - last ones are predicted labels of the test data that we are aiming to.\n",
        "We will test our model on the data set, (and that includes preprocessing test data) and in next notebooks we will compare it with pre-trained Word2Vec vectors and even better - we will use both pre-trained Word2Vec vectors and our own Word2Vec vectors and combine them for better results.\n",
        "\n"
      ],
      "metadata": {
        "id": "VkDrfHJ0VI_J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Pr6sauBJuds",
        "outputId": "945f3f73-3f56-40c3-d179-6e524e2dd760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "#Let's import all necessary tools\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "import gensim.downloader as api\n",
        "from google.colab import drive\n",
        "\n",
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "import re\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YfcxICVbpL7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This where I am going to mount my drive on google colab\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mx4vEXamhm4",
        "outputId": "8292dc33-67ce-46b6-f84b-d99cde4aef3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TdotMf7TZnY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I am changing directory , however you may do all of this on different way. \n",
        "%cd drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43zxTn0kmq38",
        "outputId": "65cec127-ae01-484e-986f-d40a05c0e65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing traning data. This traning data includes also labels. You can check all of this with df.head().\n",
        "df =pd.read_csv(\"train.csv\")"
      ],
      "metadata": {
        "id": "CgZJiJ9lmsYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I am copying all text data into one separate dataframe. I will explain why I am doing that in next few cells. \n",
        "df_text =df[['text']].copy()"
      ],
      "metadata": {
        "id": "IZjefCdX_o9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "FH34P_g4_pA2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "4a2b62fe-7f98-4f84-916b-dc5f03c31f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index  id                                              title  \\\n",
              "0      0   0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
              "1      1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
              "2      2   2                  Why the Truth Might Get You Fired   \n",
              "3      3   3  15 Civilians Killed In Single US Airstrike Hav...   \n",
              "4      4   4  Iranian woman jailed for fictional unpublished...   \n",
              "\n",
              "               author                                               text  \\\n",
              "0       Darrell Lucus  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
              "1     Daniel J. Flynn  Ever get the feeling your life circles the rou...   \n",
              "2  Consortiumnews.com  Why the Truth Might Get You Fired October 29, ...   \n",
              "3     Jessica Purkiss  Videos 15 Civilians Killed In Single US Airstr...   \n",
              "4      Howard Portnoy  Print \\nAn Iranian woman has been sentenced to...   \n",
              "\n",
              "   label  \n",
              "0      1  \n",
              "1      0  \n",
              "2      1  \n",
              "3      1  \n",
              "4      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8fefbc82-6584-42fd-b352-69180edb36e3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>Darrell Lucus</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
              "      <td>Daniel J. Flynn</td>\n",
              "      <td>Ever get the feeling your life circles the rou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Why the Truth Might Get You Fired</td>\n",
              "      <td>Consortiumnews.com</td>\n",
              "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
              "      <td>Jessica Purkiss</td>\n",
              "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
              "      <td>Howard Portnoy</td>\n",
              "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8fefbc82-6584-42fd-b352-69180edb36e3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8fefbc82-6584-42fd-b352-69180edb36e3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8fefbc82-6584-42fd-b352-69180edb36e3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df), len(df_text)"
      ],
      "metadata": {
        "id": "Et5LyE1F_pDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "676d47f2-9660-43ba-e3a2-b6d9846ff29e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20800, 20800)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This is were we are droping all rows that are empty. That can be empty title, author or text or label, and whole row will be droped. We will on this way lose about 10% rows \n",
        "#However with 2 lines of simple code we get clean data. For learning this is fair enough. \n",
        "\n",
        "df=df.dropna()\n",
        "df.reset_index(inplace=True)\n",
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzYAGiTZ5A-r",
        "outputId": "ae4471e1-2c15-4f00-bda3-639273df5233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#However as you can see, I separated data frame with text, and now I am just dropping those rows that doesn't have any text.\n",
        "#For me this is very valuable data that I will use for training my dictionary. \n",
        "df_text=df_text.dropna()\n",
        "df_text.reset_index(inplace=True)\n",
        "print(len(df_text))"
      ],
      "metadata": {
        "id": "OUjfAhhu5PJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9687bfcd-6fff-4db8-8038-53b1a82779a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "zGRcTMrBu41U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6URizoho01Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ0kv7oAvxU7",
        "outputId": "0d1ccb48-f0a9-43b1-d23d-2e81c7635b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In next cell we do a preprocessing. We are taking all 20761 text documents(rows) from df_text[\"text\"] , each with hundreds of words. We are going to clean each sentence from punctuations, numbers, and strange symbols, clean everything of the stop words, and for do lemmatization for each word in each sentence of each text. Then we append it back corpus with name \"message_text\" This process can last few minutes on your machine or google colab."
      ],
      "metadata": {
        "id": "dDelgKz_fIir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "messages_text = []\n",
        "\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "for par in df_text[\"text\"].values:\n",
        "    tmp = []\n",
        "    sentences = nltk.sent_tokenize(par)\n",
        "     \n",
        "    for sent in sentences:\n",
        "      \n",
        "        sent = sent.lower()\n",
        "        \n",
        "        tokens = tokenizer.tokenize(sent)\n",
        "         \n",
        "\n",
        "        filtered_words = [w.strip() for w in tokens if w not in STOPWORDS and len(w) > 1]\n",
        "        filtered_words2 = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
        "        tmp.extend(filtered_words2)\n",
        "    messages_text.append(tmp)"
      ],
      "metadata": {
        "id": "LBCwSEam6ICB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VecCJKlsf_T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jv80QZDhfkX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dimension of vectors we are generating\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "#Creating Word Vectors by Word2Vec Method (takes time...)\n",
        "#We are using those words to train our model. You can play with EMBEDDING_DIM but we will leave it 300 because latter we will compare it with pre-trained word2vec 300 vectors.\n",
        "#You can play with minimum occurance of each word , I ll leave it with number 3. \n",
        "word2vec_model = gensim.models.Word2Vec(sentences=messages_text, size=EMBEDDING_DIM, window=5, min_count=3)"
      ],
      "metadata": {
        "id": "nO88Y-rRwN1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model.wv.most_similar(positive=['iran'], topn = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeVVSGsAgCcb",
        "outputId": "73231221-161e-4c33-d54b-faf090d90d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('iranian', 0.6900563836097717),\n",
              " ('tehran', 0.6507840156555176),\n",
              " ('yemen', 0.6204209923744202),\n",
              " ('hizbollah', 0.6133781671524048),\n",
              " ('houthis', 0.6078612208366394),\n",
              " ('egypt', 0.604503870010376),\n",
              " ('turkey', 0.6000145673751831),\n",
              " ('sanction', 0.5976719856262207),\n",
              " ('libya', 0.5974670648574829),\n",
              " ('militarily', 0.588986873626709)]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One little trick here. You can do all preprocessing on df[\"title\"] , like I did first time. And those data latter will be our main features to predict target value(is a text a fake news or not). However, that is very few words for our model to train.  \n",
        "So I did train my model on text data."
      ],
      "metadata": {
        "id": "SpuAFpM4g_ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mrDhODnogbgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is a size of our vocab, each word in vocab has a 300 dimension vector. \n",
        "len(word2vec_model.wv.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF-_D5IJ0JS2",
        "outputId": "d82eae80-5612-48ce-d319-116a49243e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70967"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dVeIAH6AE38s",
        "outputId": "03caf96c-19c8-4692-b511-fded57ff3539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index  id                                              title  \\\n",
              "0      0   0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
              "1      1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
              "2      2   2                  Why the Truth Might Get You Fired   \n",
              "3      3   3  15 Civilians Killed In Single US Airstrike Hav...   \n",
              "4      4   4  Iranian woman jailed for fictional unpublished...   \n",
              "\n",
              "               author                                               text  \\\n",
              "0       Darrell Lucus  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
              "1     Daniel J. Flynn  Ever get the feeling your life circles the rou...   \n",
              "2  Consortiumnews.com  Why the Truth Might Get You Fired October 29, ...   \n",
              "3     Jessica Purkiss  Videos 15 Civilians Killed In Single US Airstr...   \n",
              "4      Howard Portnoy  Print \\nAn Iranian woman has been sentenced to...   \n",
              "\n",
              "   label  \n",
              "0      1  \n",
              "1      0  \n",
              "2      1  \n",
              "3      1  \n",
              "4      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae02a182-b49e-47cc-b628-30e3350872fe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>Darrell Lucus</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
              "      <td>Daniel J. Flynn</td>\n",
              "      <td>Ever get the feeling your life circles the rou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Why the Truth Might Get You Fired</td>\n",
              "      <td>Consortiumnews.com</td>\n",
              "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
              "      <td>Jessica Purkiss</td>\n",
              "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
              "      <td>Howard Portnoy</td>\n",
              "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae02a182-b49e-47cc-b628-30e3350872fe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ae02a182-b49e-47cc-b628-30e3350872fe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ae02a182-b49e-47cc-b628-30e3350872fe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now - we will preprocess all titles as well. Our goal will be to predict are news fake or true just from the titles. Computer can not read words. But it can read numbers. To turn each word into the number(300 dimension vector) we will need to preprocess all titles. "
      ],
      "metadata": {
        "id": "dMGN9WPxotJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First we will do standard cleaning. \n",
        "messages_title = []\n",
        "\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "for par in df[\"title\"].values:\n",
        "    tmp = []\n",
        "    sentences = nltk.sent_tokenize(par)\n",
        "    for sent in sentences:\n",
        "        sent = sent.lower()\n",
        "        tokens = tokenizer.tokenize(sent)\n",
        "        filtered_words = [w.strip() for w in tokens if w not in STOPWORDS and len(w) > 1]\n",
        "        filtered_words2 = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
        "        tmp.extend(filtered_words2)\n",
        "    messages_title.append(tmp)"
      ],
      "metadata": {
        "id": "X-H0iyN5GV7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages_title[0]"
      ],
      "metadata": {
        "id": "mYE2ljstGV-p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0324f10a-90aa-4f58-a796-99bdb0359a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'house dem aide even see comey letter jason chaffetz tweeted'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LcPisAIeGWBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we will connect all separated words into one string. We only use string join function from python.\n",
        "for i in range(len(messages_title)):\n",
        " messages_title[i] =\" \".join(word for word in messages_title[i])  "
      ],
      "metadata": {
        "id": "9Bd5aL_2FAB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This function is generating vector with 300 dimension vector having only ones(1). We will use it for word embedding words that are not part of our vocab.\n",
        "#To repeat, our vocab has around 70 000 words, each of words have unique 300 dimension vector that is property of that word. If some word is unknown it will get 300 dimension vector consists only of ones.\n",
        "def ones_vector(d):\n",
        "  return np.ones(d)"
      ],
      "metadata": {
        "id": "59A2QhyAJP-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this \"lista_vectora\" (list of vectors) is list that we will save all our vectors, for each row.\n",
        "lista_vectora =[]\n",
        "for row in messages_title:\n",
        "  temp_ls_vec=[]\n",
        "\n",
        "  #each word/token is split by space then - if word is in our vocab, it will get corresponding vector. If it is not, it will be generated new 300 dimension vector. \n",
        "  for token in row.split():\n",
        "    if token in word2vec_model.wv.vocab:\n",
        "      temp_ls_vec.append(word2vec_model[token])\n",
        "    else:\n",
        "      temp_ls_vec.append(ones_vector(300))\n",
        "  lista_vectora.append(temp_ls_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJcHwpyGGGP7",
        "outputId": "080e0d84-3009-4f5d-b3ee-7aea803c34d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Each title has different size. Here longest title has size of 47 words. I will use that size for padding. That means that each row in the end will have size of 47 vectors.\n",
        "\n",
        "\n",
        "max_len =[]\n",
        "for sent in lista_vectora:\n",
        "  max_len.append(len(sent))\n",
        "print(max(max_len))\n",
        "print(len(max_len))\n",
        "L = max(max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQnptuXLKEtr",
        "outputId": "2c95065e-0964-466d-ce99-32767684a651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n",
            "18285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We will use those zeros , for padding \"empty space\". As I said before, each of row will have len(47). Every word that in our trained vocab will get it's unique vector. \n",
        "#Word that is not par of our vocab will get vector of ones. All empty space will be padded with vector of zeros. \n",
        "def null_vector(num):\n",
        "  return np.zeros(num)"
      ],
      "metadata": {
        "id": "KWlbuoMoKFJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#padding of zeroes. We are creating only list of zeroes. Each row in our corpus have unique len. Maximum len is 47. Substract 47 from len or each row and you will know how much zero vectors you will create.\n",
        "padded_embeddings = []\n",
        "for row in lista_vectora:\n",
        "  temp_zer=[]\n",
        "  if len(row) <L:\n",
        "    for i in range(L-len(row)):\n",
        "      temp_zer.append(null_vector(300))\n",
        "  padded_embeddings.append(temp_zer) "
      ],
      "metadata": {
        "id": "py0zAfgBKNcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#easiest way to concat list of vectors that have only zeros(padded_embeddings) and list of vectors(lista_vectora) that have unique 300 dimension vectors for each word in vocab, and ones for unknown words\n",
        "padded_vectors= [ k+v for k,v in zip(lista_vectora ,padded_embeddings )]"
      ],
      "metadata": {
        "id": "eZG5FajTKTBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we have to convert all those numbers in numpy array.\n",
        "y_train = np.array(df[\"label\"])\n",
        "X_train = np.array(padded_vectors)"
      ],
      "metadata": {
        "id": "InGtbatoKWnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu3EdSR8UbYY",
        "outputId": "4c7c7e9d-cbfb-42f0-af58-081e20280583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18285, 47, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#every row that is going through our batch will have shape 47, 300. That will include test set latter.\n",
        "input_shape =(X_train.shape[1], X_train.shape[2])"
      ],
      "metadata": {
        "id": "9HCIYIywUz8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1aS4ejP2U_AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "rFtIFpYpUASJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Spliting data into validation and train. Test data will come latter.\n",
        "X1_train, X1_val, y1_train, y1_val  =train_test_split(X_train, y_train, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "jKmZgor_NMv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, LSTM, SpatialDropout1D, Dropout, TimeDistributed, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "6cSKtLU0UNs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xhoTipJCVXIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "# model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "    # here we do not need Embedding layer because we did it on our own) \n",
        "model.add(Input(shape=input_shape))\n",
        "    # https://keras.io/api/layers/core_layers/input/\n",
        "\n",
        "#This is my simple network that will yield some results. Goal here was more on preprocessing, embedding and how to use Word2Vec.\n",
        "#You can play with tunning \n",
        "\n",
        "model.add(LSTM(64))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(32))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v3_AhCINNG5",
        "outputId": "a3756fc3-9867-4380-975f-3134c53d7e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 64)                93440     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 95,553\n",
            "Trainable params: 95,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.fit(X1_train,y1_train, validation_data=(X1_val, y1_val), epochs=5, batch_size=32, callbacks=[EarlyStopping(monitor='val_loss', patience=2,restore_best_weights=True)] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jUHndXzVfbZ",
        "outputId": "655f0d2a-2f55-4279-90f1-b54aac2fe8be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "383/383 [==============================] - 22s 51ms/step - loss: 0.2452 - accuracy: 0.8971 - val_loss: 0.2085 - val_accuracy: 0.9122\n",
            "Epoch 2/5\n",
            "383/383 [==============================] - 19s 48ms/step - loss: 0.1740 - accuracy: 0.9242 - val_loss: 0.1802 - val_accuracy: 0.9233\n",
            "Epoch 3/5\n",
            "383/383 [==============================] - 19s 49ms/step - loss: 0.1485 - accuracy: 0.9396 - val_loss: 0.2448 - val_accuracy: 0.9229\n",
            "Epoch 4/5\n",
            "383/383 [==============================] - 19s 49ms/step - loss: 0.1354 - accuracy: 0.9438 - val_loss: 0.2083 - val_accuracy: 0.9258\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9fc42ddd10>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking folder where are our files\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0FxyMC0f5Yw",
        "outputId": "fc66ca94-c87b-4d6c-c228-dd50b82c5a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1-s2.0-S1057521915001477-main.pdf       submit.csv         X_test.npy\n",
            "\u001b[0m\u001b[01;34m'Colab Notebooks'\u001b[0m/                       test.csv           X_train_extra.npy\n",
            " \u001b[01;34mdario\u001b[0m/                                  train.csv          y_array.npy\n",
            " GoogleNews-vectors-negative300.bin.gz   X_array.csv.npy    y_test_extra.npy\n",
            " iv.docx                                 X_array.npy        y_test.npy\n",
            " spam.csv                                X_test_extra.npy   y_train_extra.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing our data. Notace that  y -values are in separated csv file(\"submit.csv\")\n",
        "df_Xtest = pd.read_csv(\"test.csv\")\n",
        "df_ytest =pd.read_csv(\"submit.csv\")"
      ],
      "metadata": {
        "id": "GEQYIQupV9gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#concating two separated test data frames so I can for example get rid of rows with missing values much easier.\n",
        "df_Xy = pd.concat([df_Xtest, df_ytest], axis=1)"
      ],
      "metadata": {
        "id": "lAb0e_uGgAvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_Xy.dropna(inplace=True)\n",
        "df_Xy.reset_index(inplace=True)\n",
        "print(len(df_Xy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf7ZVcWImA_b",
        "outputId": "69ddaad5-fb8d-43fb-d16f-19a8e0ef673f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test data must pass same process as train data. Everything must be preprocessed and tokens converted into 300 dimensional vectors.\n",
        "\n",
        "messagesX = []\n",
        "\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "for par in df_Xy[\"title\"].values:\n",
        "    tmp = []\n",
        "    sentences = nltk.sent_tokenize(par)\n",
        "    for sent in sentences:\n",
        "        sent = sent.lower()\n",
        "        tokens = tokenizer.tokenize(sent)\n",
        "        filtered_words = [w.strip() for w in tokens if w not in STOPWORDS and len(w) > 1]\n",
        "        filtered_words2 = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
        "        tmp.extend(filtered_words2)\n",
        "    messagesX.append(tmp)"
      ],
      "metadata": {
        "id": "tA3q0kAWi86I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messagesX[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXOTFrmUm_2K",
        "outputId": "951b8568-89da-4028-953f-f00be6010f6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['specter',\n",
              " 'trump',\n",
              " 'loosens',\n",
              " 'tongue',\n",
              " 'purse',\n",
              " 'string',\n",
              " 'silicon',\n",
              " 'valley',\n",
              " 'new',\n",
              " 'york',\n",
              " 'time']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(len(messagesX)):\n",
        " messagesX[i] =\" \".join(word for word in messagesX[i])  "
      ],
      "metadata": {
        "id": "7L4xMF12nBAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OORf_JAsnPRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if the word from data set is part of our vocab(that is trained on text data ) it will get it's 300 dimension vector. Otherwise it gets vector of ones.\n",
        "lista_vectora2 =[]\n",
        "for row in messagesX:\n",
        "  temp_ls_vec=[]\n",
        "  for token in row.split():\n",
        "    if token in word2vec_model.wv.vocab:\n",
        "      temp_ls_vec.append(word2vec_model.wv[token])\n",
        "    else:\n",
        "      temp_ls_vec.append(ones_vector(300))\n",
        "  lista_vectora2.append(temp_ls_vec)"
      ],
      "metadata": {
        "id": "zeJOj_a_nKRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Everything else that is \"empty space\" will be padded with zeros.\n",
        "padded_embeddings2 = []\n",
        "for row in lista_vectora2:\n",
        "  temp_zer=[]\n",
        "  if len(row) <L:\n",
        "    for i in range(L-len(row)):\n",
        "      temp_zer.append(null_vector(300))\n",
        "  padded_embeddings2.append(temp_zer) "
      ],
      "metadata": {
        "id": "LeaSOva8pLAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#as with train data, we do same with test data\n",
        "padded_vectors2= [ k+v for k,v in zip(lista_vectora2 ,padded_embeddings2 )]"
      ],
      "metadata": {
        "id": "FaxaA6roqvtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.array(df_Xy[\"label\"])\n",
        "X_test = np.array(padded_vectors2)"
      ],
      "metadata": {
        "id": "KG5zzQhLqvwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting results\n",
        "y_preds = model.predict(X_test)"
      ],
      "metadata": {
        "id": "zVmKpUo6qvz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#because I used sigmoid function for predictions, everything above 0.5 will be classifed as fake news, everything bellow 0.5 as reliable news source\n",
        "for i in range(len(y_preds)):\n",
        "  if y_preds[i] >=0.5:\n",
        "    y_preds[i]=1\n",
        "  else:\n",
        "    y_preds[i] =0"
      ],
      "metadata": {
        "id": "ZRRpIgKIrXCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets see how our data is split\n",
        "y_preds=y_preds.flatten()\n",
        "pd.Series(y_preds).value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo8SkE8ErY3X",
        "outputId": "18bfb7a7-17cb-4076-90ba-a671476f2c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    2473\n",
              "1.0    2102\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "Is9UgcAhrazw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YoF2MnmEHMe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our result. We tried to see how correct our model will be to predict is something fake news or not  just by reading titles of those news. You can get much higher result if you include author together with titles as X feature. Then model will probably learn which author is \"fake\" which real. Also you can try different type of models, different preprocessing etc. "
      ],
      "metadata": {
        "id": "6tDHW4o7HNCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(accuracy_score(y_test, y_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43BKe_OJrcpn",
        "outputId": "2225102f-f947-4fa7-874d-e585ed330866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6375956284153006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In next notebook, we will se how successfull will be our Word2Vec model that has been already pretrained on huge text."
      ],
      "metadata": {
        "id": "FbwSRoIlI9Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DpO3bvKZJBYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wfS1Mklk2Qdf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}