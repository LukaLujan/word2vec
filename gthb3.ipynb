{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gthb3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPyGb2GPebrCejjV63Oh0wa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukaLujan/word2vec/blob/main/gthb3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a third part of our text classification. Here we will do combination , we will use Word2Vec pretrained model - it has 3 million of vectors(for each word) , and we will use Word2Vec network to train it on our corpus. \n",
        "The task is simple. We will do word vectorization for words that are part of our trained model, and it some word from train/test data is not part of our vocab, we check if that word is part of a Word2Vec pretrained model and take vectors from there. If a token(word) is not part of both vocabs(our trained and pretrained) we will replace with with vector of the ones. \n",
        "We have an issue here - because this whole process is taking huge amount of RAM , we will only be able to do preprocessing here. We will convert our data to numpy array. We will do both for test data and train data. We can save that data, and load it in next colab notebook where we will have \"fresh RAM\" for our job.\n"
      ],
      "metadata": {
        "id": "fDSVYwqfhPR7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TC0HAbxaDi2",
        "outputId": "f0bf921e-3c13-4123-c971-f91f3f631d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "#most of the process is similar like in first two notebooks. So I will comment only what is different.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "import gensim.downloader as api\n",
        "from google.colab import drive\n",
        "\n",
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eUrEJv1aUj1",
        "outputId": "a2a9c21b-ebaa-45e8-84e5-628e3f4b3621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8KxVqWZgaxUV",
        "outputId": "242bfd55-f253-4b81-daf7-c43330493ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oakgZKZHbIpk",
        "outputId": "285ecd07-aeb5-4812-c0f2-fc8ff604893a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6XWgOL82azsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here I am importing  first model.\n",
        "\n",
        "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec_model1 = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "metadata": {
        "id": "iTeeS4PEbChU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df =pd.read_csv(\"train.csv\")"
      ],
      "metadata": {
        "id": "FPXzHQrac_4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_text =df[['text']].copy()"
      ],
      "metadata": {
        "id": "qBaaC98T6AVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.dropna()\n",
        "df.reset_index(inplace=True)\n",
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNrs8lXe6JMK",
        "outputId": "dd50aaf4-bf77-4321-fce0-fa9a967feda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_text=df_text.dropna()\n",
        "df_text.reset_index(inplace=True)\n",
        "print(len(df_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4OrqJ4xdJH6",
        "outputId": "59af4a61-a956-40cf-ef5c-907ab34eafa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "UM7ULIfWeS08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2IhbFQBF_VTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "wrYLcgp5_Ka5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9IdWqqL_WAD",
        "outputId": "638deffe-e3ea-4a40-d7db-de6ce8f19c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UKOPfra3A3fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This preprocessing is for the second model, that will train it's vectors on our corpus.\n",
        "\n",
        "messages = []\n",
        "\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "for par in df_text[\"text\"].values:\n",
        "    tmp = []\n",
        "    sentences = nltk.sent_tokenize(par)\n",
        "    for sent in sentences:\n",
        "        sent = sent.lower()\n",
        "        tokens = tokenizer.tokenize(sent)\n",
        "        filtered_words = [w.strip() for w in tokens if w not in STOPWORDS and len(w) > 1]\n",
        "        filtered_words2 = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
        "        tmp.extend(filtered_words2)\n",
        "         \n",
        "    messages.append(tmp)"
      ],
      "metadata": {
        "id": "h4Qp9Tuwc__y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is second model. It will be trained on our corpus.\n",
        "\n",
        "#Dimension of vectors we are generating\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "#Creating Word Vectors by Word2Vec Method (takes time...)\n",
        "word2vec_model2 = gensim.models.Word2Vec(sentences=messages, size=EMBEDDING_DIM, window=5, min_count=3)"
      ],
      "metadata": {
        "id": "pFCr2FQ2dADW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(word2vec_model1.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXiuTN0xdAGS",
        "outputId": "d1150d84-ccca-4da3-8778-e76809e95296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000000"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare the size of those.\n",
        "len(word2vec_model2.wv.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvjOxtRodAJr",
        "outputId": "85cba1bc-f969-4125-ec56-70663ad7b1ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70967"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR-d-7IGCamE",
        "outputId": "f735d3b9-cb59-4b01-ada8-47886e78865f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9032"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we are preprocessing \"titles\" that will be converted into numbers and they will go into our training model.\n",
        "#As we said before, computer doesn't recognize \"photos or words\" - it understands only numbers :) \n",
        "messages2 = []\n",
        "\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "for par in df[\"title\"].values:\n",
        "    tmp = []\n",
        "    sentences = nltk.sent_tokenize(par)\n",
        "    for sent in sentences:\n",
        "        sent = sent.lower()\n",
        "        tokens = tokenizer.tokenize(sent)\n",
        "        filtered_words = [w.strip() for w in tokens if w not in STOPWORDS and len(w) > 1]\n",
        "        filtered_words2 = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
        "        tmp.extend(filtered_words2)\n",
        "         \n",
        "    messages2.append(tmp)"
      ],
      "metadata": {
        "id": "gtHtVnYn5yy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ones_vector(d):\n",
        "  return np.ones(d)"
      ],
      "metadata": {
        "id": "_325nnk8dAT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(messages2)):\n",
        " messages2[i] =\" \".join(word for word in messages2[i])  "
      ],
      "metadata": {
        "id": "UXrcKHz7dAXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In following cell there are some changes. As you can see, I introduced \"elif\" in the loop. Logic is simple. If the token is not part of our trained model vocab, it will get a vector from pretrained model(that has 3 million of vectors). Else - vector of the ones will replace all unknown words. \n",
        "Also I introduced few new variables that will count how many vectors from each model is replaced. It will give us some insights what we can expect."
      ],
      "metadata": {
        "id": "_lsgvD5-n4qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "lista_vectora =[]\n",
        "trained_words=0\n",
        "pretrai_words=0\n",
        "ones_words   =0\n",
        "\n",
        "for row in messages2:\n",
        "  temp_ls_vec=[]\n",
        "  for token in row.split():\n",
        "    if token in word2vec_model2.wv.vocab:\n",
        "      temp_ls_vec.append(word2vec_model2.wv[token])\n",
        "      trained_words +=1\n",
        "    elif token in word2vec_model1.vocab:\n",
        "      temp_ls_vec.append(word2vec_model1[token])\n",
        "      pretrai_words +=1\n",
        "    else:\n",
        "      temp_ls_vec.append(ones_vector(300))\n",
        "      ones_words +=1\n",
        "  lista_vectora.append(temp_ls_vec)"
      ],
      "metadata": {
        "id": "-52C7xR36QIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\" number of trained words vectorizerd  {trained_words},  number of words from downloaded model {pretrai_words},  number of ones vectors {ones_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bo6D3n8TDDlf",
        "outputId": "a68dd988-d01a-464b-cba6-95c17489741c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " number of trained words vectorizerd  159001,  number of words from downloaded model 432,  number of ones vectors 1012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, most of the words were replaced by our own trained vectors(159001). Only 0.3% of the words were replaced by words from pretrained model(432) , while number of ones is 1012. How much of 0.3% new vectors will have impact on our results?"
      ],
      "metadata": {
        "id": "p9eONYGun-Lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len =[]\n",
        "for sent in lista_vectora:\n",
        "  max_len.append(len(sent))\n",
        "print(max(max_len))\n",
        "print(len(max_len))\n",
        "L = max(max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yLkb8-IcOYk",
        "outputId": "9639e0ed-9fb5-406a-8610-1a2ee37d9364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n",
            "18285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def null_vector(num):\n",
        "  return np.zeros(num)"
      ],
      "metadata": {
        "id": "tjNJpy7WauK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_embeddings = []\n",
        "for row in lista_vectora:\n",
        "  temp_zer=[]\n",
        "  if len(row) <L:\n",
        "    for i in range(L-len(row)):\n",
        "      temp_zer.append(null_vector(300))\n",
        "  padded_embeddings.append(temp_zer)  "
      ],
      "metadata": {
        "id": "8yuaab2GSrQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_vectors= [ k+v for k,v in zip(lista_vectora ,padded_embeddings )]"
      ],
      "metadata": {
        "id": "ZnZyTB1sSuLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SW1RXoZRWDpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_extra = np.array(df[\"label\"])\n",
        "X_train_extra = np.array(padded_vectors)"
      ],
      "metadata": {
        "id": "E7oc6HuCSxsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yAaUuCGNooGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are saving those numpy arrays into npy file, and this is an easy way to import it in next notebook.\n",
        "np.save(\"X_train_extra.npy\", X_train_extra)\n",
        "np.save(\"y_train_extra.npy\", y_train_extra)"
      ],
      "metadata": {
        "id": "_z334oDCS2Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8VlRslmyonld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r5W8eQZQonox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#increasing RAM on google colab!! we are deleting all variables that we don't need."
      ],
      "metadata": {
        "id": "oMgfdqJEWF-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del y_train_extra, X_train_extra, messages, messages2, df, padded_vectors, lista_vectora, padded_embeddings"
      ],
      "metadata": {
        "id": "HwfUY3ioWGBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD3R3AC-XkyI",
        "outputId": "3b299dbd-b198-4b00-c07e-faab4f1998e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1-s2.0-S1057521915001477-main.pdf       submit.csv         X_test.npy\n",
            "\u001b[0m\u001b[01;34m'Colab Notebooks'\u001b[0m/                       test.csv           X_train_extra.npy\n",
            " \u001b[01;34mdario\u001b[0m/                                  train.csv          y_array.npy\n",
            " GoogleNews-vectors-negative300.bin.gz   X_array.csv.npy    y_test_extra.npy\n",
            " iv.docx                                 X_array.npy        y_test.npy\n",
            " spam.csv                                X_test_extra.npy   y_train_extra.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we are doing preprocessing of the test data as well!\n",
        "df_test =pd.read_csv(\"test.csv\")"
      ],
      "metadata": {
        "id": "nrrZxoZUWqvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_t= pd.read_csv(\"submit.csv\")"
      ],
      "metadata": {
        "id": "Jr3HmFEVXskP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pd.concat([df_test, y_t], axis=1)"
      ],
      "metadata": {
        "id": "UBDOU4cQWudQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvoPK_i4YyVE",
        "outputId": "3db55f78-ec5f-467e-e37a-80201316ad1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5200"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "YzMxEZxpYsuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMWIPh9LY1Su",
        "outputId": "83671339-a72c-4d8f-8c83-e478cf052d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4575"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messagesX = []\n",
        "\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "for par in result[\"title\"].values:\n",
        "    tmp = []\n",
        "    sentences = nltk.sent_tokenize(par)\n",
        "    for sent in sentences:\n",
        "        sent = sent.lower()\n",
        "        tokens = tokenizer.tokenize(sent)\n",
        "        filtered_words = [w.strip() for w in tokens if w not in STOPWORDS and len(w) > 1]\n",
        "        filtered_words2 = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
        "        tmp.extend(filtered_words2)\n",
        "    messagesX.append(tmp)"
      ],
      "metadata": {
        "id": "6d0P-O-eYttY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messagesX[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Okru8of9aF3K",
        "outputId": "a4c4e6db-1868-4b3a-ef50-3a60dca45ef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['specter',\n",
              " 'trump',\n",
              " 'loosens',\n",
              " 'tongue',\n",
              " 'purse',\n",
              " 'string',\n",
              " 'silicon',\n",
              " 'valley',\n",
              " 'new',\n",
              " 'york',\n",
              " 'time']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(messagesX)):\n",
        " messagesX[i] =\" \".join(word for word in messagesX[i])  "
      ],
      "metadata": {
        "id": "HyqRZTPkaDdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_words=0\n",
        "pretrai_words=0\n",
        "ones_words   =0\n",
        "lista_vectora2 =[]\n",
        "for row in messagesX:\n",
        "  temp_ls_vec=[]\n",
        "  for token in row.split():\n",
        "    if token in word2vec_model2.wv.vocab:\n",
        "      temp_ls_vec.append(word2vec_model2.wv[token])\n",
        "      trained_words+=1\n",
        "    elif token in word2vec_model1.vocab:\n",
        "      temp_ls_vec.append(word2vec_model1[token])\n",
        "      pretrai_words +=1\n",
        "    else:\n",
        "      temp_ls_vec.append(ones_vector(300))\n",
        "      ones_words +=1\n",
        "  lista_vectora2.append(temp_ls_vec)"
      ],
      "metadata": {
        "id": "jI-yC0WLaUTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(trained_words, pretrai_words, ones_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lzh2qy4ZbJue",
        "outputId": "15d7f786-f71c-4fe2-d8c9-4826fb119eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39130 164 442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_embeddings2 = []\n",
        "for row in lista_vectora2:\n",
        "  temp_zer=[]\n",
        "  if len(row) <L:\n",
        "    for i in range(L-len(row)):\n",
        "      temp_zer.append(null_vector(300))\n",
        "  padded_embeddings2.append(temp_zer)"
      ],
      "metadata": {
        "id": "ADg2mmBvbVKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_vectors2= [ k+v for k,v in zip(lista_vectora2 ,padded_embeddings2 )]"
      ],
      "metadata": {
        "id": "ypE26OvwcrMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AOpuzhS5pbzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting test data into numpy arrays\n",
        "y_test_extra = np.array(result[\"label\"])\n",
        "X_test_extra = np.array(padded_vectors2)"
      ],
      "metadata": {
        "id": "hyPbwKJ0cvWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "L6gmNiEPpbIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_test_extra) == len(X_test_extra)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDjw_KUJcv87",
        "outputId": "09d71fdd-da08-402e-ef71-7c46df6c22ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#saving test data for next notebook.\n",
        "np.save(\"X_test_extra.npy\", X_test_extra)\n",
        "np.save(\"y_test_extra.npy\", y_test_extra)"
      ],
      "metadata": {
        "id": "arh1X6Tldo3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_extra.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LR6Fcl3Gsls",
        "outputId": "d50a1aa1-66f4-42b3-dbd4-a261b2faed2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4575, 47, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "f6mk5QqVG-Ly"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}